{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "Hypothesis testing or significance testing is a method for testing a claim or hypothesis about a parameter in a population, using data measured in a sample. It is generally composed by the following steps:\n",
    "\n",
    "- it's common to start defining the **null hypothesis** ($H_0$), which represents the simplest case, and initially presume it to be true. An **alternative hypothesis** ($H_1$) is a statement that directly contradicts a null hypothesis by stating something that is different to what has been stated in the null hypothesis. The hypotheses $H_0$ and $H_1$ necessarily have to be exclusive, e.g. they cannot be both true.\n",
    "\n",
    "- subsequently, we set the criteria for a decision by setting the level of significance for a test. The level of significance set by convention at 5% in in several scientific fields, but it could also be different (e.g. 10%)\n",
    "\n",
    "- the key part is computing the **test statistic**. The test statistics is a properly defined scalar variable to quantify, within observed data, the features that would distinguish the null from the alternative hypothesis.\n",
    "\n",
    "- the probability of obtaining a certain value of the test statistics, given that the null hypothesis is true, is stated by the **p-value**. The p-value is a probability that varies between 0 and 1 and can never be negative. The p-value can be easily calculated if its distribution of the test statistics is *known in advance* (few and lucky cases), otherwise it has to be determined, usually by performing pseudo-experiments. \n",
    "\n",
    "- finally, we make a decision about the null hypothesis by using the value of the test statistic that is obtained from the data. The decision is based on the probability of obtaining a certain value, given that the null hypothesis is true. If the p-value is smaller than the level of significance (e.g. 5%), then the decision is to reject the null hypothesis. If the probability is larger, then the decision is to retain the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The $Z$ test\n",
    "\n",
    "The $Z$ test is one of the simplest methods for statistical testing. It has limited applications, as it is applicable only to the cases when the test statistics are approximately **normally distributed** for large samples, and **the variance of the population is known**. If the population variance is unknown (and therefore has to be estimated from the sample itself) or the sample size is not large ($n \\lesssim 30$), the Student's t-test (see later) may be more appropriate.\n",
    "\n",
    "In summary, the $Z$ test requires the following:\n",
    "\n",
    "- the expected value $\\mu$ of the test statistics under the null hypothesis\n",
    "- the standard deviation $\\sigma$ of the test statistics under the null hypothesis\n",
    "- having determined if the test statistics is one-tailed or two-tailed\n",
    "- choosing the significance level $\\alpha$ (usually $0.05$, but could be different)\n",
    "\n",
    "Then, the test score $Z$ is calculated as:\n",
    "\n",
    "$$Z = \\frac{\\bar{x} - \\mu}{\\sigma}$$\n",
    "\n",
    "The $Z$ score is then compared to the standard normal distribution cumulative probability, in order to find the probability of observing a standard normal value larger (in absolute value) than the observed $Z$ score. This is the *p-value*.\n",
    "\n",
    "The fact that the test statistics is one- or two-tailed will imply that the two-sided p-value is approximately twice the one-sided p-value. \n",
    "\n",
    "Finally, reject the null hypothesis if the p-value is below your significance level $\\alpha$.\n",
    "\n",
    "\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Let's pretend that we want to determine if an object is made of pure gold by measuring its melting point. We know the probability distribution for gold melting is normal with mean 1060 $^{\\circ}$C and our measurements have a standard deviation of 3$^{\\circ}$C. We take one jewel, and it melts at 1045 $^{\\circ}$C. We want to know if we should be suspicious about that jewel.\n",
    "\n",
    "Let's see if we can disprove the null hypothesis (the jewel is made of gold). In this case, it's safe to assume that the temperature measurements are normally distributed and we know the variance; also, the test statistics is two-sided, as the measured temperature can be either larger or smaller than the mean value. Let's calculate the $Z$ score using the normal distribution in `scipy.stats.norm`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt, pi, erf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "x0 = 1045.\n",
    "mu = 1060.\n",
    "sigma = 3.\n",
    "\n",
    "# determine the Z score\n",
    "Z = abs(x0 - mu) / sigma\n",
    "print(\"Z =\", Z)\n",
    "\n",
    "# plot the measurement and its assumed pdf\n",
    "w = mu - x0\n",
    "x = np.linspace(mu - w - 3, w + mu + 3, 1000)\n",
    "y = stats.norm.pdf(x, loc=mu, scale=sigma)\n",
    "plt.plot(x, y)\n",
    "plt.fill_between(x, y, where=np.abs(x - mu) > w, color='lightblue')\n",
    "plt.axvline(x0, linestyle='--', color='red')\n",
    "plt.axvline(mu, linestyle='--', color='lightblue')\n",
    "plt.ylabel(\"$p(x)$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the test statistics and its pdf\n",
    "xs = np.linspace(-6, +6, 1000)\n",
    "ts = stats.norm.pdf(xs)\n",
    "plt.plot(xs, ts)\n",
    "plt.fill_between(xs, ts, where=np.abs(xs) > np.abs(Z), color='lightblue')\n",
    "plt.axvline(Z, linestyle='--', color='orange')\n",
    "plt.axvline(-Z, linestyle='--', color='blue')\n",
    "plt.ylabel(\"a.u.\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()\n",
    "\n",
    "# calculate the p-value as the sum of the left tail + the right tail\n",
    "pvalue = stats.norm.cdf(-Z) + (1. - stats.norm.cdf(Z))\n",
    "# note that you can surely perform an integral of stats.norm.pdf, but using the cdf is much more convenient\n",
    "print(\"p-value =\", pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The probability to observe a jewel melting at temperatures smaller than 1045$^{\\circ}$C or larger than 1075$^{\\circ}$C is then $\\approx 6 \\cdot 10^{-7}$, which is smaller then $\\alpha$. So it's safe to reject the null hypothesis (the jewel is made of gold), and be legitimately suspicious of the jewel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Student's $t$-Test\n",
    "\n",
    "The Student's t-test is a statistical test that is used to compare the means of two data samples. In other words, it determines a probability that two populations are the same with respect to the variable tested.\n",
    "\n",
    "It is often used in hypothesis testing to determine whether a process or treatment actually has an effect on the population of interest, or to check if the mean of a distribution significantly differs from the expected value.\n",
    "\n",
    "The main difference with the $Z$ test is that the variance of the sample is unknown.\n",
    "\n",
    "The t-test is a parametric test of difference, meaning that it makes the same assumptions about the input data samples:\n",
    "\n",
    "- are independent\n",
    "- are (approximately) normally distributed\n",
    "- have a similar amount of variance within each group\n",
    "\n",
    "If your data does not fit these assumptions, you have to resort to more complex, nonparametric tests.\n",
    "\n",
    "Since the variance of the data sample is not known a-priori, the variance is estimated directly from the sample:\n",
    "\n",
    "$$\\sigma^2 = \\frac{\\sum_i^n (x_i - \\bar{x})^2}{n-1}$$\n",
    "\n",
    "and the test statistics becomes:\n",
    "\n",
    "$$T = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}}$$\n",
    "\n",
    "where $\\bar{x}$ is the sample mean of size $n$, and $\\mu$ is the population mean or the expected value.\n",
    "\n",
    "The distribution of the test statistics $T$ given the number of degrees of freedom $ndof = n-1$ is known under the name of T Student distribution, and is implemented in SciPy in `stats.t`.\n",
    "\n",
    "Knowing the distribution of the test statistics allows to determine the p-value by finding $\\int_{-T}^T p(t)\\,dt$ (in case of double-side p-values).\n",
    "\n",
    "To be precise, the $t$-test and $Z$-test require normality of the sample means, and the $t$-test additionally requires that the sample variance follows a scaled $\\chi^2$ distribution, and that the sample mean and sample variance are statistically independent. By the central limit theorem, sample means of large samples are often well-approximated by a normal distribution even if the data are not normally distributed. For non-normal data, the distribution of the sample variance may deviate substantially from a $\\chi^2$ distribution, but the if the sample size is large, Slutsky's theorem implies that the distribution of the sample variance has little effect on the distribution of the test statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Example:\n",
    "\n",
    "In a similar same case as in the previous example, we have some jewels, all of the same material, and we want to verify that they are made of gold by measuring the melting temperature. All we know is that the probability distribution for gold melting is normal with mean 1060 $^{\\circ}$C; we don't know the standard deviation.\n",
    "A person brings in 6 samples and they melt at 1035, 1050, 1020, 1055, and 1046$^{\\circ}$C. Should we reject the null hpyothesis (they are made of gold)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "samples = np.array([1035., 1050., 1020., 1055., 1046.]) # input data\n",
    "sigma = np.sqrt(np.var(samples, ddof=1)) # calculate the variance and its sqrt()\n",
    "sample_mean = np.mean(samples) # mean of the sample (which is different from mu)\n",
    "mu = 1060. # the expected value for the mean\n",
    "n = len(samples) # the size of the data sample\n",
    "w = mu - sample_mean\n",
    "\n",
    "# calculate the test statistics\n",
    "T = (sample_mean - mu) / (sigma / np.sqrt(n))\n",
    "print(\"T =\", T)\n",
    "\n",
    "# plot the\n",
    "x = np.linspace(mu - w - 5, w + mu + 5, 1000)\n",
    "y = stats.t.pdf(x, loc=mu, scale=sigma / np.sqrt(len(samples)), df=len(samples) - 1)\n",
    "plt.plot(x, y)\n",
    "plt.axvline(sample_mean, linestyle='--', color='red')\n",
    "plt.axvline(mu, linestyle='--', color='lightblue')\n",
    "plt.xticks([mu - w, mu + w], [mu - w, mu + w])\n",
    "plt.ylabel(\"$p(x)$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.show()\n",
    "# note that this is NOT a plot of the test statistics! This is a plot of the measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the test statistics and its pdf\n",
    "xs = np.linspace(-5, +5, 1000)\n",
    "ts = stats.t.pdf(xs, n - 1)\n",
    "plt.plot(xs, ts)\n",
    "plt.fill_between(xs, ts, where=np.abs(xs) > np.abs(T), color='lightblue')\n",
    "plt.axvline(T, linestyle='--', color='orange')\n",
    "plt.axvline(-T, linestyle='--', color='blue')\n",
    "plt.ylabel(\"a.u.\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.show()\n",
    "\n",
    "# now we calculate the p-value. Since it's double sided:\n",
    "pvalue = stats.t.cdf(T, n - 1) + (1. - stats.t.cdf(-T, n - 1))\n",
    "print(\"p-value =\", pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the p-value is not $< \\alpha/2$, we accept the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical integration\n",
    "\n",
    "Numerical integration, also called quadrature (especially in one-dimension), adresses the basic problem in numerical integration to compute an approximate solution to a definite integral $\\int_a^b f(x) dx$ to a given degree of accuracy.\n",
    "\n",
    "$f(x)$ should be a smooth function integrated over a small number of dimensions, and the domain of integration should be bounded.\n",
    "\n",
    "There are many methods for approximating the integral to the desired precision, which potentially require one whole course per se, which are implemented in the `scipy.integrate` Pythion library. In the following examples, we will see the application of the `integrate` library to solve some common cases.\n",
    "\n",
    "The most generic integration routine is `scipy.integrate.quad()`, which integrate a one-dimensional function from $a$ to $b$ using a technique (check the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html)) from the Fortran library QUADPACK.\n",
    "\n",
    "In this example, we compute the integral of a simple function $\\int_0^{\\pi/2} \\sin{\\theta}d\\theta$, whose result is known analytically (and is 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad, quadrature\n",
    "\n",
    "# call quad and pass the function, the min and max of the integration interval\n",
    "# note that the function must be numpy-compliant, e.g. vectorized\n",
    "# in other words, only numpy functions (np.cos, ...) and vectorized operations (+, -, *, **, ...) are allowed\n",
    "res, err = quad(np.sin, 0, np.pi/2)\n",
    "\n",
    "# an alternative method is the gaussian quadrature\n",
    "#res, err = quadrature(np.sin, 0, np.pi/2)\n",
    "\n",
    "print(res, \"+-\", err)\n",
    "print(np.allclose(res, 1)) # np.allclose() checks that two numpy arrays have all close values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of integration of a not-so-simple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_function(x):\n",
    "    if x < 0:\n",
    "        return np.exp(x)\n",
    "    if x < 3:\n",
    "        return x**2\n",
    "    if x == 3:\n",
    "        return 0\n",
    "    if x > 3:\n",
    "        return np.sqrt(x)\n",
    "    if x >= 5:\n",
    "        return np.exp(-x)\n",
    "    \n",
    "# this numpy method converts the regular function into a numpy function\n",
    "np_conditional_function = np.vectorize(conditional_function)\n",
    "\n",
    "res, err = quad(np_conditional_function, -4, 6) #Integrate it from -4 to 6 using the numpy version\n",
    "print(res, \"+-\", err)\n",
    "\n",
    "x = np.arange(-4, 6, 0.01)\n",
    "plt.plot(x, np_conditional_function(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more generic function, but less user-friendly, that extends `quad` to multiple dimensions is `nquad` ([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.nquad.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import nquad\n",
    "\n",
    "# define functions as lambda\n",
    "func = lambda x0, x1, x2, x3 : x0**2 + x1*x2 - x3**3 + np.sin(x0) + (1 if (x0 - .2*x3 - .5 - .25*x1 > 0) else 0)\n",
    "# define integration points\n",
    "points = [[lambda x1, x2, x3 : 0.2*x3 + 0.5 + 0.25*x1], [], [], []]\n",
    "# define optional parameters\n",
    "def opts0(*args, **kwargs):\n",
    "    return {'points' : [0.2*args[2] + 0.5 + 0.25*args[0]]} \n",
    "\n",
    "result, abserr, out = nquad(func, [[0,1], [-1,1], [.13,.8], [-.15,1]], opts=[opts0,{},{},{}], full_output=True)\n",
    "\n",
    "print(result, \"+-\", abserr)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Ordinary Differential Equations\n",
    "\n",
    "The Python `scipy.integrate` library also features routines for integrating Ordinary Differential Equations (ODE). It cannot handle coupled (PDEs) or boundary problems. For those and all other things not in scipy, see this list of other [numerical methods](https://www.scipy.org/topical-software.html).\n",
    "\n",
    "To solve an ODE in Python (and all other programming languages/packages):\n",
    "\n",
    " - Convert to standard form\n",
    " - Implement the standardized equation as a Python function\n",
    " - Create a grid of points where you want to evaluate the ODE\n",
    " - Call `scipy.integrate.odeint()` passing the function, initial value, and grid\n",
    " \n",
    "Specifically, `scipy.integrate.odeint()` solves ODE of the form:\n",
    "\n",
    "$$\\frac{dy}{dt} = f(y1, y2, .., t0, ...)$$\n",
    "\n",
    "All first order, second order and n-th order ODEs can be converted into this form. Let's see some examples.\n",
    "\n",
    "\n",
    "### First order ODEs\n",
    "\n",
    " - $\\frac{dC(t)}{dt}=−rC(t)$ (chemical reaction)\n",
    " - $R\\frac{di(t)}{dt} + \\frac{i(t)}{C} = 0$ (RC circuits)\n",
    " - $L\\frac{di(t)}{dt} + R i(t) = V$ (RL circuits)\n",
    " \n",
    "these are a 1-st order ODE, already in standard form or that can be expressed in the standard form with trivial operations.\n",
    "\n",
    "As an example, let us solve the ODE $\\frac{dy}{dt} = -2 y$ between $t = 0 \\dots 4$, with the initial condition $y(t=0) = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint\n",
    "\n",
    "# first the function computing the derivative of the position needs to be defined\n",
    "def derivative(ypos, time):\n",
    "    return -2 * ypos\n",
    "\n",
    "# define the interval\n",
    "time_vec = np.linspace(0, 4, 40)\n",
    "\n",
    "# call the odeint() method\n",
    "y = odeint(derivative, y0=1, t=time_vec)\n",
    "\n",
    "# plot the function\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(time_vec, y)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second order ODEs\n",
    "\n",
    "- $L\\frac{d^2 i(t)}{dt^2} + R\\frac{di(t)}{dt} + \\frac{1}{C}i(t) = 0$ (RLC circuit)\n",
    "- $\\frac{d^2 x(t)}{dt^2} + k x(t) = 0$ (non-damped mass-spring oscillation)\n",
    "- $\\frac{d^2 x(t)}{dt^2} + 2 \\varepsilon \\omega_0  \\frac{dx(t)}{dt} + \\omega_0^2 x(t) = 0$ (mass-spring damped oscillation)\n",
    "\n",
    "The \"trick\" for the 2-nd order ODEs is to create a second dimension, the dimensions of the derivative, which turns the problem into a 2D 1-st order ODE.\n",
    "\n",
    "#### Example: the mass-spring system\n",
    "\n",
    "In the case of the non-damped spring mass system, we'll use a $x_1(t)=x(t)$ and $x_2(t)=\\frac{dx(t)}{dt}$.\n",
    "So the equation becomes:\n",
    "\n",
    "$$\\frac{x_2(t)}{dt} + k x_1 (t) = 0$$\n",
    "and thus\n",
    "$$\\frac{x_2(t)}{dt} = - k x_1 (t)$$\n",
    "$$\\frac{x_1(t)}{dt} = x_2(t)$$\n",
    "Now we have two dimensions and their ODEs are both in standard form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: the damped mass-spring system\n",
    "\n",
    "Let's see a solution for the damped spring mass system.\n",
    "The position of a mass attached to a spring obeys the 2-nd order ODE reported above, with $\\omega_0^2 = k/m$ with $k$ the spring constant, $m$ the mass and $\\varepsilon = c/(2 m \\omega_0)$ with $c$ the damping coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the constants\n",
    "mass = 0.5  # kg\n",
    "kspring = 4  # N/m\n",
    "cviscous = 0.4  # N s/m\n",
    "\n",
    "# and thus\n",
    "eps = cviscous / (2 * mass * np.sqrt(kspring/mass))\n",
    "omega = np.sqrt(kspring / mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2-nd order equation needs to be transformed in a system of two first-order equations for the vector $Y = (y, y')$. This can be done by exploiting 2D numpy arrays. The function computes the velocity and acceleration. In this case, the two 1-st order ODEs are:\n",
    "\n",
    "$$\\frac{x_1(t)}{dt} = x_2(t)$$\n",
    "$$\\frac{x_2(t)}{dt} = - \\varepsilon \\omega_0 x_1 (t) - \\omega_0^2 x_2(t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(yvec, time, eps, omega):\n",
    "    return(yvec[1], -eps * omega * yvec[1] - omega**2 * yvec[0])\n",
    "\n",
    "time_vec = np.linspace(0, 10, 100)\n",
    "yinit = (1, 0)\n",
    "yarr = odeint(derivative, yinit, time_vec, args=(eps, omega))\n",
    "\n",
    "# plot y and y'\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(time_vec, yarr[:, 0], label='y')\n",
    "plt.plot(time_vec, yarr[:, 1], label=\"y'\")\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('A')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Fourier Trasform\n",
    "\n",
    "The Fourier Transform (FT) is a mathematical transform that decomposes functions that depend on space or time into functions depending on spatial or temporal frequency.\n",
    "\n",
    "It is often used to filter the noise from a periodic signal and investigate features in the frequency space.\n",
    "\n",
    "The `scipy.fftpack` module computes fast Fourier transforms (FFTs) and offers utilities to handle them. The main functions are:\n",
    "\n",
    "* `scipy.fftpack.fft()` to compute the FFT\n",
    "* `scipy.fftpack.fftfreq()` to generate the sampling frequencies\n",
    "* `scipy.fftpack.ifft()` computes the inverse FFT, from frequency space to signal space\n",
    "\n",
    "Let's see a practical example, generating a periodic signal (using a `np.sin` function) and adding on top some random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "\n",
    "time_step = 0.05\n",
    "ang_freq = 0.2\n",
    "period = 1. / ang_freq\n",
    "# define the time range\n",
    "time_vec = np.arange(0, 20, time_step)\n",
    "# define the function as signal + noise\n",
    "sig = (np.sin(2 * np.pi / period * time_vec) + 0.5 * np.random.randn(time_vec.size))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(time_vec, sig, label='Original signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute and plot the power spectrum using `fftpack.fft` ([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.fftpack.fft.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The FFT of the signal\n",
    "sig_fft = fftpack.fft(sig)\n",
    "\n",
    "# And the power (sig_fft is of complex dtype)\n",
    "power = np.abs(sig_fft) # these are the \"weights\" of the Fourier components for each discrete frequency interval\n",
    "\n",
    "# The return the corresponding frequencies\n",
    "sample_freq = fftpack.fftfreq(sig.size, d=time_step)\n",
    "\n",
    "# Plot the FFT power\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(sample_freq, power)\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Power')\n",
    "\n",
    "# Find the peak frequency: focus only on only the positive frequencies\n",
    "pos_mask = np.where(sample_freq > 0)\n",
    "freqs = sample_freq[pos_mask]\n",
    "powes = power[pos_mask]\n",
    "# find the max of freqs array\n",
    "peak_freq = freqs[powes.argmax()]\n",
    "print(\"Peak frequency:\", peak_freq)\n",
    "\n",
    "# Check that it does indeed correspond to the frequency that we generate the signal with\n",
    "print(\"Does the peak correspond to the initial frequency?\", np.allclose(peak_freq, ang_freq))\n",
    "\n",
    "# An inner plot to show the peak frequency\n",
    "axes = plt.axes([0.55, 0.3, 0.3, 0.5])\n",
    "plt.title('Peak frequency')\n",
    "plt.plot(freqs[:8], powes[:8])\n",
    "plt.setp(axes, yticks=[])\n",
    "\n",
    "# scipy.signal.find_peaks_cwt can also be used for more advanced peak detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create a filter that removes the random noise and returns the periodic signal. We can observe that the random noise populates the high frequency part of the spectrum, so we may want to remove these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_freq_fft = sig_fft.copy()\n",
    "# filter (set to zero) all high frequency components that are larger than peak_freq\n",
    "high_freq_fft[np.abs(sample_freq) > peak_freq] = 0\n",
    "# calculate the Inverse Fast Fourier Transform\n",
    "filtered_sig = fftpack.ifft(high_freq_fft)\n",
    "# only take the real part\n",
    "real_filtered_signal = np.real(filtered_sig)\n",
    "\n",
    "# plot the result of the IFFT\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(time_vec, sig, alpha=0.5, label='Original signal')\n",
    "plt.plot(time_vec, real_filtered_signal, linewidth=3, label='Filtered signal')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
